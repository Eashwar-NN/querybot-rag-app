# ğŸ“„ QueryBot: Distributed AI Data Platform

QueryBot is a production-ready, distributed Retrieval-Augmented Generation (RAG) platform. Unlike simple scripts or prototypes, QueryBot implements an asynchronous, event-driven architecture that decouples data ingestion from user querying, ensuring high availability and scalability.

It features a full ingestion pipeline, a persistent vector database, and an automated quality evaluation harness using "LLM-as-a-Judge."

## âœ¨ Key Features

* **ğŸš€ Asynchronous Ingestion Pipeline**: Uploads are handled instantly via a Redis job queue. Heavy processing (chunking, embedding) happens in a background worker without blocking the API.
* **ğŸ—„ï¸ Persistent Data Layer**:
  * **MinIO**: S3-compatible object storage for raw PDF files.
  * **ChromaDB**: Persistent, Dockerized vector database for embedding storage.
* **âš¡ Stateless API**: The FastAPI backend is completely stateless, allowing for horizontal scaling.
* **ğŸ§  Automated Evaluation**: Includes a built-in harness using **Ragas** to quantitatively score the pipeline on *Faithfulness* and *Answer Relevancy*.
* **ğŸ³ Fully Containerized**: The entire stack (API, Worker, Frontend, Redis, MinIO, ChromaDB) is orchestrated via Docker Compose.

## ğŸ› ï¸ Tech Stack

**Core Infrastructure:**
* **Orchestration**: Docker Compose
* **Queue**: Redis
* **Object Storage**: MinIO
* **Vector Database**: ChromaDB

**Application Layer:**
* **Backend API**: FastAPI
* **Background Worker**: Python (custom script)
* **Frontend**: Streamlit
* **LLM Integration**: LangChain
* **Local LLM**: Ollama (Llama 3)
* **Embeddings**: Sentence-Transformers (`all-MiniLM-L6-v2`)

**Quality Assurance:**
* **Evaluation Framework**: Ragas (Retrieval Augmented Generation Assessment)
* **Synthetic Data**: LangChain + Ollama

## ğŸ“‚ Project Structure

```text
querybot/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py               # Stateless FastAPI server (Query Engine)
â”‚   â”œâ”€â”€ worker.py             # Background Worker (Ingestion Engine)
â”‚   â”œâ”€â”€ preload.py            # Model pre-loader for Docker build
â”‚   â”œâ”€â”€ Dockerfile            # Shared Docker image for API and Worker
â”‚   â””â”€â”€ requirements.txt      # Python dependencies
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ app.py                # Streamlit Dashboard
â”‚   â”œâ”€â”€ Dockerfile            # Frontend Docker image
â”‚   â””â”€â”€ requirements.txt      # Frontend dependencies
â”œâ”€â”€ evaluation/               # QA & Testing Harness
â”‚   â”œâ”€â”€ gen_testset.py        # Generates synthetic test questions from PDFs
â”‚   â”œâ”€â”€ evaluate.py           # Runs Ragas metrics using "LLM-as-a-Judge"
â”‚   â””â”€â”€ transformer.pdf       # Document for testing
â”œâ”€â”€ docker-compose.yml        # Defines the 6-container cluster
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
````

## ğŸš€ Getting Started

### Prerequisites

1.  **Docker Desktop**: Installed and running.
2.  **Ollama**: Installed on your host machine with the Llama 3 model pulled.
    ```bash
    ollama run llama3:8b
    ```
3.  **Python 3.10+**: (Optional) Required only if you want to run the local evaluation scripts.

### Running the Platform

1.  **Clone the repository:**

    ```bash
    git clone <your-repository-url>
    cd querybot
    ```

2.  **Launch the Cluster:**
    This command builds the images and starts all services (API, Worker, Frontend, Database, Queue, Storage).

    ```bash
    docker-compose up --build
    ```

3.  **Access the Services:**

      * **User Interface**: http://localhost:8501
      * **MinIO Console**: http://localhost:9001 (User: `miniouser`, Pass: `miniopassword`)
      * **API Docs**: http://localhost:8001/docs

## ğŸ“ Usage

### 1\. Ingestion (Upload)

  * Go to the Streamlit UI.
  * Upload a PDF document.
  * **What happens:** The API uploads the file to MinIO and pushes a job to Redis. The UI returns "Accepted" immediately.
  * **Background:** The Worker wakes up, processes the PDF, and saves vectors to ChromaDB. Watch the Docker logs to see this happen in real-time\!

### 2\. Querying

  * Type a question in the UI.
  * **What happens:** The API searches ChromaDB for relevant context and sends it to Ollama to generate an answer.

### 3\. Quality Evaluation (Local)

To rigorously test the bot's accuracy, you can run the evaluation suite locally.

1.  **Install Dev Dependencies:**

    ```bash
    pip install langchain langchain-community langchain-ollama pypdf ragas pandas
    ```

2.  **Generate Test Data:**
    Place your PDF (e.g., `transformer.pdf`) inside the `evaluation/` folder and run:

    ```bash
    cd evaluation
    python gen_testset.py
    ```

    This creates `testset.json` with synthetic questions.

3.  **Run the Grader:**

    ```bash
    python evaluate.py
    ```

    This runs your test set against the running API and uses a local LLM judge to score Faithfulness and Relevancy.

## âš™ï¸ API Endpoints

  * **POST `/upload`**: Asynchronous endpoint. Uploads file to storage and queues processing job. Returns `202 Accepted`.
  * **POST `/query`**: Synchronous endpoint. Accepts `{question: str}`, performs vector search, and returns `{answer: str, context: list}`.
